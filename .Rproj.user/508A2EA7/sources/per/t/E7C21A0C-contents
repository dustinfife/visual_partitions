---
title: "Visual Partitioning for Multivariate Models: An Approach for Identifying and
  Visualizing Complex Multivariate Dataset"
shorttitle: "Visual Partitioning"
author: 
  - name      : "Dustin A. Fife"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "201 Mullica Hill Road
                    Glassboro, NJ 08028"
    email         : "fife.dustin@gmail.com"
  - name      : "Jorge L. Mendoza"
    affiliation   : "2"

affiliation:
  - id            : "1"
    institution   : "Rowan University"
  - id            : "2"
    institution   : "University of Oklahoma"    
abstract: |
  Users of statistics quite frequently use multivariate models to make conditional inferences (e.g., stress affects depression, after controlling for gender). These inferences are often done without adequately considering (or understanding) the assumptions one makes when claiming these inferences. Of particular concern is when there are unmodeled nonlinear and/or interaction effects. With such unmodeled multiplicative effects, inferences based on a main effects model are not merited. On the other hand, when these effects are properly modeled, complex multivariate analyses can be "partitioned" into distinct components to ease interpretation. In this paper, we highlight when conditional inferences are contaminated by other features of the model and identify the conditions under which effects can be partitioned. We also reveal a strategy for partitioning multivariate effects into uncontaminated blocks using visualizations. This approach simplifies multivariate analyses immensely, without oversimplifying the analysis.
documentclass     : "apa6"
classoption       : "doc"
output            : 
  papaja::apa6_pdf:
    number_sections: false
bibliography: 
- all_references.bib
header-includes:
- \usepackage{amsmath}
- \usepackage{tikz}
- \usetikzlibrary{shapes.geometric,arrows, positioning}
- \usepackage[LGRgreek]{mathastext}
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE, note=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, note=FALSE, cache=FALSE)
options(knitr.kable.NA = '')
require(tidyverse)
require(flexplot)
```

It is human nature to seek to use cognitive heuristics to simplify decision-making. While these heuristics reduce effort, they can often lead to bias [@Tversky1974]. This is, perhaps, no more apparent in scientific research than with data analysis. While attempting to make sense of complex data structures, researchers routinely rely on simple heuristics, such as *p* < 0.05 for significance testing, RMSEA less than 0.05 for structural equation models [@browncudeck; @joreskog1993lisrel], judging difference between groups by whether confidence intervals overlap [@Coulson2010], or the various formulae for determining sample sizes [see @Aguinis1 for a review].

The problem with these heuristics is they inevitably oversimplify the nuances of data analysis and decision-making. Perhaps it is overly optimistic to believe that the message of thousands of datapoints can be adequately summarized by a single statistic or rule of thumb. Rather, it is commonly recognized that statistical summaries are poor representations of statistical models [e.g., see @Czerlinski1999; @McShane2017; @schmidt_statistical_1996 for criticisms of $p$-values; and see @Steiger2007 for criticisms on SEM fit indices]. 

```{r anscombe, fig.cap="A reproduction of Anscombe's quartet. Each plot has identical regression lines/correlations, even though the underlying data are vastly different.", fig.width=4, fig.height=2.5, out.width="60%", fig.align="center"}
require(tidyverse)
require(flexplavaan)
xs = anscombe %>% 
  pivot_longer(x1:x4, names_to="group", values_to="x", names_pattern="x(.*)") %>% 
  dplyr::select(group, x)
ys = anscombe %>% 
  pivot_longer(y1:y4, names_to="group", values_to="y", names_pattern="y(.*)") %>% 
  dplyr::select(y)
library(extrafont)
theme_update(text = element_text(family = "Times"))
d = cbind(xs, ys)
ggplot(d, aes(x=x, y=y)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE, fullrange=T) +
  facet_grid(~group, ) +
  theme_bw() + 
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )
```
  
One of the most striking problems with these simple summaries is that there is a "many-to-one" relationship between statistical summaries and raw data; vastly different raw datasets can yield identical summary statistics. This may be best illustrated by "Anscombe's Quartet" [@Anscombe1973a; see @matejka2017same for a more modern -- and entertaining -- illustration of Anscombe's quartet], shown in Figure \@ref(fig:anscombe). Each of these four plots show identical regression lines with identical correlation coefficients, yet the data that generated these statistical summaries are vastly different. 

Perhaps the best (if not, only) way to satisfy the human need for simplicity without increasing bias is through data visualization. The human pattern-recognition system is well-equipped for many types of visuals. Indeed, over half of the human brain is devoted to visual processing [@halfbrain; @sheth1996orientation]. Visuals allow us to, at a glance, determine the strength of a statistical relationship, identify sources of misfit and evaluate model assumptions [@Healy2014a; @Levine2018], and improve encoding of statistical information [@Correll2015]. As noted by @Wilkinson1999a, "If you assess hypotheses without examining your data, you risk publishing nonsense" (p. 597).

For these reasons, many scholars insist all statistical models must have accompanying graphics that visualize the statistical model [e.g., @Fife2019e; @fife2021graph; @Pastore2017; @Tay2016a; @Wilkinson1999a]. Unfortunately, this is easier said than done, at least for analyses that contain more than two or three variables; while it is common and intuitive to visualize each variable on an axis (e.g., when performing a t-test or regression), this strategy becomes much less effective once one attempts to visualize more than two dimensions (e.g., with a factorial ANOVA or ANCOVA). 

This is not to say researchers haven't tried in the past. For example, one strategy is to utilize three-dimensional plots, where users can rotate the perspective of the graphic. A more sophisticated approach to 3D visualization is found in the software GGobi [@swayne2011ggobi], which uses perpetual rotation to attempt to visualize more than three dimensions. Other approaches to multivariate [^mv] visualizations include scatterplot matrices, clustering algorithms like multidimensional scaling and heat maps, or encoding additional dimensions using alternative features (e.g., color, symbols, or panels). Each of these approaches have their strengths and weaknesses. In this paper, we favor an eclectic approach to multivariate visualization. This approach will cobble together various approaches into a systematic strategy that aims to simplify understanding of complex multivariate visualizations. The end result of this strategy is a small collection of visually independent plots that capture the entirety of the multivariate relationships, without introducing bias of interpretation. 

[^mv]: Among statisticians, the term "multivariate" refers to analyses which require multiple *de*pendent variables, while for non-statisticians, multivariate refers to analyses which utilize multiple *in*dependent variables. For this paper, we adopt the latter terminology. 

However, before we illustrate this strategy, we begin by reviewing assumptions of multivariate linear models. In doing so, we will show how common approaches to multivariate analyses often suffer from bias, while also laying the foundation for how to identify visually independent components of a multivariate model. Subsequently, we introduce three visualization tools we use extensively for multivariate visualizations: Flexplot, marginal plots, and partial residual plots (PRPs). We then show how these tools can be combined to visualize multivariate data, then we highlight our strategy's utility with an applied example.

# Assumptions of Multiple Regression Models

It is well-known that regression (and multiple regression) subsumes the t-test, ANOVA, ANCOVA, factorial ANOVA, etc. [@Cohen1968]. For this reason, we will use the term "multiple regression" to refer to any analysis which utilizes a single numeric (i.e., interval or ratio) outcome variable and two or more predictor variables, regardless of whether the predictor variables are nominal, ordinal, interval, or ratio. The strategy we introduce will apply to any of these procedures, which covers the vast majority of analyses researchers utilize. 

In order to make inferences and/or to have unbiased estimates, multiple regression models typically make five key assumptions: 

1. Normality of the residuals. This assumption states that the residuals of a statistical model form an approximately normal distribution. 
2. Homoscedasticity. This assumption states that the variance at every level of the predictor variable(s) is approximately constant. 
3. Independence. This assumption states that scores within a dataset (e.g., Person One and Person Two's scores) are not influenced by one another. 
4. Linearity. This assumption states that the functional form of the predictor(s)/outcome form a straight line, or at least that any nonlinearity is explicitly modeled. 
5. Homogeneity of regression. This assumption states that all interactions between variables have been explicitly modeled. 

Generally, multiple regression (MR) models are quite robust to violations of the first two assumptions [for a review, see @maxwell_designing_2004].[^centering] Violating the third assumption (independence) is quite problematic, but this violation is easily fixed by adopting different models (e.g., mixed models or repeated-measures ANOVAs). Likewise violating the latter two assumptions (linearity and homogeneity of regression) is extremely problematic [@gelman2006data][^lin], which we illustrate in the next section. Fortunately, these latter two problems are easily fixed by simply modeling the nonlinearity (e.g., through a quadratic term) or by adding appropriate interaction effects to the model.

[^lin]: Gelman and Hill (2006) add "validity" to their list of assumptions. This assumption states that the measures of the variables are valid. They also note that this is the most important of all assumptions, meaning that violation of this assumption is most problematic. (We agree). Also according to Gelman and Hill, the next most egregious assumptions are what they call "additivity and linearity," which are the same assumptions as linearity and homogeneity of regression. 

[^centering]: It is common practice to center variables when multiplicative effects are modeled. While centering can improve one's ability to interpret slopes [@cohen_applied_2013; @dalal_common_2012], and (arguably) may reduce collinearity between the predictors [@dalal_common_2012], they have no effect on the visuals we introduce, except for changing the range of the axes. For this reason, we will say no more about centering variables. [See also @OlveraAstivia2018; @echambadi_mean-centering_2007].


```{r}
# suicidal ideation = stress (nonlinear), health (regular ME), friend_ideation*depression
library(tidyverse)
library(flexplot)
covmat = matrix(c(
  1, .3, .3, 0, .6,
  .3, 1, -.2, .2, .3,
  .3, -.2, 1, .1, -.3,
  0, .2, .1, 1, .3,
  .6, .3, -.3, .3, 1
), nrow=5)
d = MASS::mvrnorm(3000, mu=c(0,0,0,0,0), Sigma=covmat) %>% 
  data.frame %>% 
  set_names(nm=c("ideation", "stress", "health", "friend_ideation", "depression")) %>% 
  mutate(ideation = ideation + .5*scale(stress)^2 + .4*scale(friend_ideation)*scale(depression)) %>% 
  mutate(ideation = rescale(ideation, 20, 3),
         stress = rescale(stress, 10, 2),
         health = rescale(health, 30, 6),
         friend_ideation = rescale(friend_ideation, 20, 3), 
         depression = rescale(depression, 15, 4))
```

# Nonlinearity and Interactions Bias Interpretation

Let us assume we have the following MR model: 

\begin{align}
\label{eq:model} \text{suicide ideation}=b_0 +& b_1 \text{health} \\
\nonumber                              +& b_2 \text{stress} + b_3 \text{stress}^2 \\
\nonumber                              +& b_4 \text{friend ideation} + b_5 \text{depression} + b_6 \text{friend ideation} \times \text{depression} 
\end{align}


\noindent In other words, the health effect is linear, the stress effect is nonlinear, and depression/friend ideation interact with one another. For simplicity, we assume all variables are continuous, but they need not be. In fact, the approach we introduce is actually simpler with categorical variables. (With some visuals, numeric variables must be artificially categorized, which is not required for categorical variables). 

Now, suppose instead of fitting the model shown in Equation \@ref(eq:model), we instead fit a simple linear, non-interactive model:

\begin{align}
\label{eq:model2}  \text{suicide ideation}=b_0 +& b_1 \text{health} \\
\nonumber                              +& b_2 \text{stress} \\
\nonumber                              +& b_4 \text{friend ideation} + b_5 \text{depression}
\end{align}

Naturally, we might ask what the consequences are of violating the assumptions of linearity and homogeneity of regression. Figure \@ref(fig:ignorequad) illustrates what would happen if we ignored the nonlinearity in the data. The fitted model, which doesn't model the nonlinearity, is illustrated with the red line. This model suggests a negative relationship between stress and ideation. In other words, according to this model, reducing stress *always* reduces suicide ideation. However, the correct model (illustrated as a blue line) shows that decreasing stress actually *increases* suicide ideation for those with stress values less than approximately 10. Not only are the implications of these models problematic from a statistical perspective, but they're extremely problematic from a practical perspective: the wrong conclusions could cost lives. 
    

```{r ignorequad, fig.cap="This graph shows the relationship between suicide ideation and stress, both for a model that ignores the nonlinear component (red line) and a model that fits a nonlinear component (blue).", fig.width=5, fig.height=3.5, out.width="60%", fig.align="center"}
modwrong = lm(ideation~stress + depression + friend_ideation + health, data=d)
flexplot(ideation~stress, data=d, method="quadratic") + 
  geom_abline(slope=coef(modwrong)[2], intercept = coef(modwrong)[1] + 
                coef(modwrong)[3]*mean(d$depression) +
                coef(modwrong)[4]*mean(d$friend_ideation) +
                coef(modwrong)[5]*mean(d$health), col="red") +
  labs(x="Stress", y="Suicidal Ideation") + theme_bw()
```

What happens if we ignore the interaction effect? Figure \@ref(fig:ignoreint) shows the fit of the wrong model (left plot) versus the fit of the correct model (right plot). The wrong model suggest there's no relationship between one's suicide ideation and the suicide ideation of one's friends. On the other hand, the correct model states that the influence of a friend's ideation *depends* on one's depression; having a suicidal friend actually decreases ideation for those low in depression, while it increases ideation for those with high depression. Once again, the incorrect model has both statistical and practical consequences. Statistically, the wrong model will introduce bias. Practically, the wrong model may lead one to falsely ignore the influence of an individuals' friends. 

```{r ignoreint, fig.cap="This graph shows the relationship between suicide ideation and the suicide ideation of one's best friend. The left plot shows the fit for a model that ignores the interaction, while the right plot shows the fit of the model that includes the interaction.", fig.width=7, fig.height=4, fig.align="center", out.width="75%"}

a = flexplot(ideation~friend_ideation, data=d, method="lm") +
  labs(x="Friend's Suicidal Ideation", y="Suicidal Ideation") + theme_bw() 
b = flexplot(ideation~friend_ideation | depression, data=d, method="lm", labels=list("depression" = c("low", "medium", "high"))) +
  labs(x="Friend's Suicidal Ideation", y="Suicidal Ideation") + theme_bw()
library(patchwork)
a+b + plot_layout(widths = c(2, 3))
```

One could, of course, avoid these issues by modeling all nonlinear and interactive effects. Unfortunately, the researcher never knows whether nonlinear effects are present. Traditionally, one might model all possible interactions and nonlinear terms explicitly, then use statistical estimates such as $p$-values to, one by one, dismiss nonlinear and interactive terms. However, this approach becomes quite intractable for even small models. Suppose a researcher has $k$ variables. The number of possible quadratic terms is equal to $k$, but the number of interaction components is equal to: 

$$kC_2 + kC_3 + ... + kC_k $$

\noindent In total, the number of terms (main effects, quadratic effects, and interaction effects) could plausibly equal:

$$ k \text{(main effects)} + k \text{(quadratic effects)} + kC_2 + ... + kC_k$$

\noindent (This is assuming all nonlinear effects are quadratic, rather than cubic [for example], *and* this assumes the nonlinear terms do not interact. Either of these assumptions -- or both -- could easily be violated, as we will see in the example.)

Suppose our analysis has four variables. In total, we would have a minimum of $4 + 4 + 6 + 4 + 1 = 19$ possible terms, and the total number of possible models that include interactive and/or quadratic effects is $19 + 19C_{18} + 19C_{17} + .... + 19C_2 = 524,287$. If one were to try to identify which terms should be nonlinear or multiplicative using $p$-values (or even AIC, BIC, partial $R^2$, etc), the model could quickly become overly complicated and/or capitalize on chance. 

Deciding among these 524,287 models has historically been done using stepwise regression or machine learning (e.g., random forest). It is commonly known that stepwise regression models are quite prone to capitalizing on chance [@hendersonstepwise; @thompson]. Machine learning algorithms fair much better, but they have steep learning curves. Also, even if one were to utilize machine learning, these algorithms still do not identify the nature of the relationships. They might tell the user which variables are important, but not how exactly they're associated with the outcome variable. 

In summary, MR models are very common in psychological research. These models require one to meet the assumptions of linearity and homogeneity of regression, yet statistical assumptions are rarely checked [@hoekstra_are_2012], and probably violated regularly [@Cronbach1975; @Lord1968]. Evaluating these assumptions is both tedious and prone to capitalizing on chance. 

This paper offers an intuitive strategy for evaluating these assumptions. This strategy doesn't require users to specify all possible interactions or nonlinear terms, nor does it have a steep learning curve. Additionally, this strategy relies on visualizations, which are intuitive to interpret and protect against problematic model assumptions. In short, this strategy balances the need for simplicity, while avoiding cognitive and statistical biases. Before we introduce this strategy, we will discuss the tools required to perform these analyses. 

# Visualization Tools 

Visualizing nonlinear relationships is simple; one simply needs to utilize a scatterplot and (optionally) overlay a nonlinear line (e.g., quadratic, cubic spline, or loess line). There are also many tools to visualize interaction effects. One approach is a "simple-slopes" analysis [@cohen_applied_2013], where the user picks a few relatively arbitrary points on the moderator (e.g., +1 SD, mean, and -1 SD), then shows the regression lines for the other predictor variable at each of these levels of the moderator. While this approach is both intuitive and easy to perform, it suffers from several weaknesses: the points chosen (i.e., $\pm$ 1 SD/mean) may occur where data are quite sparse [@Finsaas2020]; these plots do not show raw data, which makes it harder to assess model assumptions [@McCabe2018]; and the choice of which predictor variable is considered the moderator is arbitrary [@Finsaas2020]. Scholars have proposed alternative methods, including 3-D planes [@Finsaas2020; @cohen_applied_2013], the Johnson-Neyman technique [@Finsaas2020; @Johnson1936; @Bauer2005], and paneled plots [@McCabe2018]. Most of these methods are effective for the purposes for which they were designed (i.e., to visualize an already-identified interaction), while many will not serve the purposes of this paper. 

```{r flexplotexample, fig.cap="These plots show examples of Flexplot graphics. In the top image, a numeric variable (injuries) is binned then displayed as separate panels. In the bottom image, injuries is binned, but displayed as separate lines/colors/symbols.", fig.align="center", fig.width=7, fig.height=5.5, out.width="70%"}
a = flexplot(ptsd~willpower | injuries, data=avengers, method="lm") + labs(x="Willpower", y="PTSD")
b = flexplot(ptsd~willpower + injuries, data=avengers, method="lm") + labs(x="Willpower", y="PTSD")
a+b + plot_layout(nrow=2)
```

In this paper, we assume the user does not know whether (or which) variables might interact. Put differently, most existing methods were designed for more confirmatory or post-hoc methods, while our approach is designed for exploratory/rough confirmatory analysis [@Fife2019a]. Because probabilistic inferences such as $p$-values and confidence intervals are essentially meaningless for exploratory research [@Fife2019a], we will not address probability-based issues, such as computing the statistical significance of interaction terms, or estimating confidence intervals for simple slopes analyses. [For those interested in these topics, see @cohen_applied_2013; @McCabe2018; @preachermargineffects].

```{r flexplotequation, fig.cap="This image shows the suicide ideation data with a Flexplot graphic. Stress is shown on the $X$-axis, health is separated into bins and different values are shown as colors/symbols/lines, and friend ideation/depression are also binned and displayed in separate panels.", fig.align="center", fig.width=6, fig.height=4, out.width="75%"}
flexplot(ideation~stress + health | friend_ideation + depression, data=d,
         labels=list("friend_ideation" = c("low", "medium", "high"),
                     "health_binned" = c("low", "medium", "high"),
                     "depression" = c("low", "medium", "high"))) + 
  labs(x="Suicide Ideation", y="Stress", color="Health", linetype="Health", shape="Health") + theme_bw()
ideation = d

```

## Tool \#1: Flexplot

The first tool we will utilize is Flexplot [@Fife2019c]. Flexplot was designed to provide an easy-to-use interface for statistical modeling. With one-line functions, users can visualize univariate, bivariate, and multivariate relationships. Flexplot is available as an R package [@Fife2019c], as well as a point-and-click interface in both JASP [@JASPTeam2019] and Jamovi [@Jamovi2018]. 

In Flexplot, the foundation of nearly all graphics is a simple bivariate scatterplot. Additional variables are encoded using either separate panels (as in the top image in Figure \@ref(fig:flexplotexample)) or separate colors/symbols/lines (as in the lower image in Figure \@ref(fig:flexplotexample)). 

When using Flexplot in R, relationships can be specified using a Flexplot equation of the following form: $y = a + b | c + d$. The variable $y$ is displayed on the $Y$-axis, $a$ is on the $X$-axis, $b$ is displayed as separate colors/symbols/lines, $c$ is shown in row panels, and $d$ is shown in column panels. As an example, consider Figure \@ref(fig:flexplotequation). This plot shows the same simulated data as before, but visualized with Flexplot. The equation used to generate these data was `ideation~stress + health | friend_ideation + depression`. 

Flexplot defaults to overlaying "loess" lines [@Cleveland1993], which are nonparametric smoothing functions which are allowed to bend with the data. The reason for this default is that loess lines will reveal any nonlinearities present. As such, Flexplot is well-equipped to identify nonlinearities. Also, Flexplot makes it easy to identify interaction effects; interactions will show up as nonparallel lines across either panels or colors/lines/symbols. However, with so many panels it is easy to experience information overload and fail to identify patterns of nonparallel lines. The next tool, however, makes it easy to identify nonparallel lines in a Flexplot graphic. 

## Tool \#2: Marginal Plots

The second tool we will utilize is what we call a "marginal plot." These too are available in the R package `flexplot` (as of version 0.10). These plots are different than "marginal-effects plots" [@berry2012improving; @preachermargineffects], which plots the slope of $X$ against the moderator ($Z$). Marginal plots make it easy to detect interactions. 

```{r threeinteractions1, fig.cap="This shows an example of a marginal plot where $X$ interacts with $Z$ only. The marginal plot shows non-parallel slopes across the columns.", fig.align="center", fig.width=6, fig.height=4, out.width="75%"}
n = 1200
x = rnorm(n)
z = rnorm(n)
w = rnorm(n)
y1 = .3*x + .3*z + .3*w + .5*x*z + rnorm(n, 0, .5)
y2 = .3*x + .3*z + .3*w + .5*x*w + rnorm(n, 0, .5)
y3 = .3*x + .3*z + .3*w + .7*x*z*w + rnorm(n, 0, .5)#+ .5*x*z + .5*x*w + .5*w*z + rnorm(n, 0, .5)
y4 = .3*x + .3*z + .3*w + .7*x*z*w + .5*x*z + .5*x*w + .5*w*z + rnorm(n, 0, .5)
d = data.frame(x, w, z, y1, y2, y3, y4)
a = marginal_plot(
  flexplot(y1~x | z + w, data=d, method="lm", 
                           labels=list("x" = c("low", "medium", "high"),
                                       "z" = c("low", "medium", "high")
                                       )) + 
    labs(x="x", y="y", title="Interaction Between X and Z") )
a
```

Recall that Flexplot allows one to plot multivariate relationships (as in Figure \@ref(fig:flexplotequation)). These plots can be used to detect interactions by identifying whether the slopes across each of the panels (or across the different lines/colors/symbols) are parallel. If the lines are not parallel, one of the variables (either the variable in the row panel, the variable in the column panel, or the variable represented as a color/line/symbol) interacts with the variable on the $X$-axis. More specifically, 

* if lines systematically deviate from parallel as a function of the columns, the variable on the $X$-axis interacts with the variable in column panels
* if lines systematically deviate from parallel as a function of the rows, the variable on the $X$-axis interacts with the variable in row panels
* if lines systematically deviate from parallel as a function of the colors/lines/symbols, the variable on the $X$-axis interacts with the variable represented as a color/line/symbol
* if lines systematically deviate from parallel as a function of rows and/or columns, there's possibly a three or four-way interaction

While the rules are simple, it becomes difficult to visually aggregate across panels (or colors/lines/symbols) using Flexplot alone. To assist with this visual aggregation, marginal plots produce three additional plots: the plots above the Flexplot will show the average slope for each column panel, averaged across the row panels. The plots to the right of the Flexplot will show the average slope for each row panel, averaged across the column panels. The top-right plot will show the average slope for $X$, averaged across both rows and columns. (Unfortunately, marginal plots do not average across colors/symbols/lines). 

```{r threeinteractions2, fig.cap="Example of a marginal plot where $X$ interacts with $W$ only. The marginal plot shows non-parallel slopes across the rows.", fig.align="center", fig.width=6, fig.height=4, out.width="75%"}
marginal_plot(
  flexplot(y2~x | z + w, data=d, method="lm", 
           labels = list("x" = c("low", "medium", "high"),
                "z" = c("low", "medium", "high")))
           + theme_bw() + labs(x="x", y="y", title="Interaction Between X and W"))
```

Figures \@ref(fig:threeinteractions1)-\@ref(fig:threeinteractions3) shows examples of several of these conditions. The first plot (Figure \@ref(fig:threeinteractions1)) shows an interaction between $X$ and the variable in the column panels ($Z$). Notice that the row plots seem to be fairly parallel, while the column plots increase their slope as $Z$ increases. The second plot (Figure \@ref(fig:threeinteractions2)) shows an interaction between $X$ and the variable in the row panels ($W$). Now, the column plots are fairly parallel, while the row plots go from negative to positive. Finally, (Figure \@ref(fig:threeinteractions3)) shows a three-way interaction. Here, the marginal plots suggest the rows and columns are parallel. However, notice that, within the 9-panel grid of the raw data, the slopes are *not* parallel. For these particular data, the data were simulated to have a three-way interaction, but no two-way interactions. Alternatively, both the row and the column margins might be nonparallel, which might indicate the variables have both two-way and three-way interactions present. 

```{r threeinteractions3, fig.cap="Example of a marginal plot where $X$ interacts with $W$ and $Z$. The marginal plot shows parallel slopes across the rows and panels, but the raw data in the 9x9 grid show nonparallel lines.", fig.align="center", fig.width=6, fig.height=4, out.width="75%"}
a = marginal_plot(flexplot(y3~x | z + w, data=d, method="lm",
                           labels=list("x" = c("low", "medium", "high"),
                                       "z" = c("low", "medium", "high")))+ 
                    labs(x="x", y="y"))
a
```



## Tool \#3: Partial Residual Plots

Partial residual plots [@Ezekiel; @larsenMcCleary] are another useful (though rarely used) tool for data analysis. The goal of partial residual plots (PRPs) is to visualize the conditional effect of a variable (i.e., the effect of a predictor variable, after controlling for another predictor or multiple predictors). For simplicity, we will call the variable we wish to control the "covariate" and the variable we wish to determine the effect the "interest variable."

Suppose we have a statistical model of the form $y = b_0 + b_1 X + b_2 Z$. Further suppose we consider $X$ to be our covariate and $Z$ to be our interest variable. We know that the parameter associated with $Z$ ($b_2$) is to be interpreted as the slope of $Z$, *after controlling for the effects of $X$*, or *holding $X$ constant.* How does one visualize this effect?

PRPs are one approach for visualizing the net effect of a variable. PRPs were invented by @Ezekiel, then independently reinvented by @larsenMcCleary. To plot a PRP, we:

1. Fit the entire statistical model (for example, $y = b_0 + b_1 X + b_2 Z$). 
2. Calculate the residuals from Step #1. We will call these residuals $e_{Y|X + Z}$. 
3. Calculate the residuals with only the $X$ effect removed (but not the $Z$ effect). We will call these residuals $e_{Y|X \neg Z}$. To calculate these, we add the fitted values for $Z$ back to the residuals: $e_{Y|X \neg Z} = e_{Y|X + Z} + \beta_Z \times Z$. 
4. Plot the $Z$ variable against these new residuals ($e_{Y|X \neg Z}$). 

When this is done, the slope of the plotted line is *exactly* equal to $b_2$. As such, these are a precise visual representation of the statistical model. 

<!-- \begin{figure} -->
<!-- \begin{center} -->
<!-- \begin{tikzpicture}[node distance=2.5cm, auto,>=latex, thick, scale = 2, minimum width=3.75cm,minimum height=1.5cm, rounded corners=0.15cm] -->
<!-- \node[draw, rectangle] (s1) {Fit $y = b_0 + b_1 X + b_2 Z$}; -->
<!-- \node[draw, rectangle] (s2) [below of=s1] {Calculate residuals ($e_{X+Z}$)}; -->
<!-- \node[draw, rectangle] (s3) [below of=s2] {Add fit back to $e_{Y|X+Z}$ to get $e_X$}; -->
<!-- \node[draw, rectangle] (s4) [below of=s3] {Plot  $e_{Y|X}$ against $Z$}; -->
<!-- \node (s5) [below of=s4] {}; -->

<!-- \draw[->] (s1) -- (s2); -->
<!-- \draw[->] (s2) -- (s3); -->
<!-- \draw[->] (s3) -- (s4); -->
<!-- \end{tikzpicture} -->
<!-- \caption{Step by step process of creating partial residual plots. Here, $y$ is the outcome variable, $X$ is the covariate, and $Z$ is the interest variable.} -->
<!-- \label{fig:prpprocess} -->
<!-- \end{center} -->
<!-- \end{figure} -->

PRPs are enormously helpful for visualizing multivariate relationships simply because they convert a complex multivariate relationship (e.g., the model $y = b_0 + b_1 X + b_2 Z$) into a simple bivariate scatterplot. However, this simplification is *only* merited if the statistical model doesn't contain interactions or nonlinear terms. Fortunately, the traditional PRP can be slightly modified to accommodate situations where these effects are present. In the next section, we introduce a multivariate extension of the PRPs that will accomplish this goal. 

### Multivariate Extensions of PRPs

The multivariate extension of the PRP follows the same procedure as the original approach. Now, however, the user can specify more complex statistical models in Step #1 (e.g., $y = b_0 + b_1 X + b_2 Z +b_3 X^2 + b_4 X\times Z$). Also, we can choose to plot multiple variables against the residuals (e.g., plot $X$ on the $X$-axis, $W$ in panels, and the remaining residuals on the $Y$-axis). Now, however, one would add back in the residuals of multiple variables (e.g., $X$ *and* $W$) in Step #3. 

This can all be done in the Flexplot package in R (as of version 10.0.2.1), using the function `partial_residual_plot`. To do so, the function requires four arguments: 

1. A Flexplot equation that specifies how variables are to be plotted. As before, we use an equation of the form `y~a+b|c+d`, or any of its variations (e.g., `y~a|b`, which places $b$ in panels, or `y~a+b`, which plots different lines/colors/symbols for different levels of $b$).
2. The multivariate model. This model may contain any number of predictors, nonlinear terms, and/or interaction components. As of this writing, the `partial_residual_plot` function requires the model to be an `lm` model, though future versions of Flexplot may extend this functionality to additional models (e.g., `glm`, or `rlm` models). 
3. Which terms in #2 are going to be added back to the residuals from the model. Note that the terms added back must involve the same variables used in Step 1 (e.g., if one specifies `y~a|b` in Step 1, the terms added back might be `a + a^2 + b + a:b`, each of which involves the variables `a` and `b`.)
4. The dataset
 
```{r prp1a, echo=FALSE, fig.cap="This plot shows a multivariate partial residual plot of the friend ideation/depression relationship, conditional on stress and health. I = ideation, S = Stress, H = Health.", fig.width=7, fig.height=5, out.width="70%", fig.align="center"}
model = lm(ideation~stress + 
             I(stress^2)+ 
             depression * friend_ideation + 
             health, 
           data=ideation)
partial_residual_plot(ideation~friend_ideation | depression, 
                      model=model, 
                      added_term = ~depression*friend_ideation, 
                      data=ideation,
                      labels=list("depression" = c("low", "medium", "high"))) +
  # modify the labels
  ylab(expression(I==S~+~S^{2}~+~H)) + 
  xlab("Friend Ideation") +
  theme_bw()
``` 
    
The code below demonstrates how to do this in R. This code first fits the entire model from Equation \@ref(eq:model) (where ideation is predicted from health, stress, a quadratic stress term, depression, friend ideation, and the interaction between friend ideation and depression). Subsequently, we use the `partial_residual_plot` function to specify the Flexplot equation for graphing (`ideation~friend_ideation|depression`), which will plot friend_ideation on the $X$-axis and depression in panels. We also specify the `added_term` argument, which specifies which terms to add back into the residuals (in this case `~depression*friend_ideation`, which will add back to the residuals the main effects and interactions from depression and friend_ideation). Additional arguments are added for visual clarity (including labeling the axes). The resulting plot is shown in Figure \@ref(fig:prp1a)

```{r prp1, echo=TRUE, eval=FALSE}
require(flexplot)
# fit the multivariate model
model = lm(ideation~stress + 
             I(stress^2)+ 
             depression * friend_ideation + 
             health, 
           data=ideation)
# plot the PRP
partial_residual_plot(ideation~friend_ideation | depression, 
                      model=model, 
                      added_term = ~depression*friend_ideation, 
                      data=ideation) +
  # modify the labels
  ylab(expression(I==S~+~S^{2}~+~H)) + 
  xlab("Friend Ideation") +
  theme_bw()
```

Now, suppose we wished to visualize the stress effect. As before, we might pass the fitted multivariate model to the `partial_residual_plot` function as before, but our Flexplot formula would be `ideation~stress`. We also tell the function to add the terms `~stress + I(stress^2)` back to the residuals of the model. The code below produces the plot in Figure \@ref(fig:prp2):
  
```{r prp2, echo = TRUE, fig.cap="This plot shows a multivariate partial residual plot of the stress relationship, conditional on health, depression, and friend ideation. In this PRP, we add back the linear and nonlinear effects of stress. I = ideation, S = Stress, H = Health, D = Depression, FI = Friend Ideation.", fig.width=5, fig.height=3, fig.align="center", out.width="75%"}
# plot stress on the x-axis, and remove it's linear and nonlinear components
partial_residual_plot(ideation~stress, 
                      model=model, 
                      added_term = ~stress + I(stress^2), 
                      data=ideation) +
  # modify the labels
  ylab(expression(I==H~+~D~+~FI~+~{D}%*%{FI})) + 
  xlab("Stress") +
  theme_bw()

```

### Residual Analysis with PRPs

Our multivariate extension of PRPs are useful, not only because they are able to accurately visualize the conditional statistical effects, but they're also useful for identifying *unmodeled* effects. To do so, one could leave the `added_term` argument blank, which will tell the `partial_residual_plot` function to simply plot the residuals (instead of adding back an effect to the residuals). In this situation, these plots are similar (if not identical) to residual dependence plots[^rdps], which are often used to identify missing nonlinear components.

[^rdps]: Residual dependence plots show the fitted values on the $X$-axis and the residuals of the model on the $Y$-axis. They are frequently used in model diagnostics to see whether there are unmodeled relationships between the fit of the model and the residuals (e.g., if there's linearity that wasn't modeled). Residual dependence plots are a simple bivariate plot (fitted on $X$-axis, residuals on $Y$-axis). If one produces a bivariate graphic with a PRP without specifying any values in the `added_term` argument, PRPs are exactly equal to the residual dependence plot. However, PRPs can also plot multivariate plots, or (as we said previously) add back fitted elements into the plot. As such, residual dependence plots are a "special case" of PRPs. 

For example, suppose we have a model where we fit the linear component of stress, but not the nonlinear component. If we then plot stress as a PRP without adding any terms back to the model, the raw data will display a nonlinear relationship (which indicates that our model failed to capture the nonlinear effect of stress on the outcome). Figure \@ref(fig:missingnonlin) shows that result. These sorts of residual analysis plots are a critical component of the visualization strategy we recommend. 

```{r missingnonlin, fig.cap="This figure shows a PRP of stress against the model that includes the linear, but not the nonlinear terms of stress. In this situation, PRPs are identical to partial residual plots, which highlight sources of unmodeled nonlinearity.", fig.width=5, fig.height=3, fig.align="center", out.width="75%"}

model = lm(ideation~stress + 
             depression * friend_ideation + 
             health, 
           data=ideation)
partial_residual_plot(ideation~stress, 
                      model=model,
                      data=ideation) +
  ylab(expression(I==H~+~D~+~FI~+~{D}%*%{FI})) + 
  xlab("Friend Ideation") + 
  theme_bw()

```

# Obtaining Conditionally Independent Effects

As we have demonstrated, if one ignores interactions and/or nonlinear effects, statistical models will be misleading. Interpreting linear effects of a predictor will be misleading if that variable has a nonlinear relationship with the outcome. Likewise, interpreting main effects when they have interactions will bias interpretation [@Appelbaum1974]. Put differently, when a variable enters into more than one term in an analysis, any interpretation of the "main (linear) effect" of that variable is misleading. 

The same applies for visualizations: it is necessary to plot each variable's *total* effect. For variables with quadratic terms, that means we must plot both the linear and nonlinear effects together. Likewise, for interactions, we must plot each variable alongside the other variable(s) with which they interact. 

On the other hand, once we have visualized each variable in the context of its multiplicative effects (i.e., interaction variables alongside interaction variables and nonlinear terms simultaneously with linear terms), the model's effects can be visualized independently without bias. Put differently, we can have (conditionally or marginally) independent visual representations of the model. For example, if our true model is of the form of Equation \@ref(eq:model), we only need three plots to capture and interpret the entire multivariate model: a plot of the health main effect, a plot of the linear + nonlinear stress effect, and a plot of the friend ideation/depression effect. Figure \@ref(fig:allplotstogether) shows a visual representation of the model in its entirety. 

```{r allplotstogether, fig.cap="The entire multivariate model in Equation 1 can be visualized using only three PRPs. The top-left plot shows the health effect, the top-right plot shows the stress nonlinear effect, and the bottom plot shows the depression/friend ideation interaction.", fig.align="center", fig.width=6, fig.height=6, out.width="75%"}

model = lm(ideation~stress + 
             I(stress^2)+ 
             depression * friend_ideation + 
             health, 
           data=ideation)
a = partial_residual_plot(ideation~health, 
                      model=model, 
                      added_term = ~health, 
                      data=ideation) +
  ylab(expression(I==S~+~S^{2}~+~D~+~FI~+~{D}%*%{FI})) + 
  xlab("Health") + 
  theme_bw()
b = partial_residual_plot(ideation~stress, 
                      model=model, 
                      added_term = ~stress + I(stress^2), 
                      data=ideation) +
  ylab(expression(I==S~+~S^{2}~+~H)) + 
  xlab("Stress") + 
  theme_bw()
c = partial_residual_plot(ideation~friend_ideation | depression, 
                      model=model, 
                      added_term = ~depression*friend_ideation, 
                      data=ideation, labels=list("depression" = c("low", "mid", "high"))) +
  ylab(expression(I==S~+~S^{2}~+~H)) + 
  xlab("Friend Ideation") + 
  theme_bw()
(a + b) / c 

```

The strategy we introduce aims to find these conditionally independent visual representations. We call this strategy "visual partitioning," because it partitions the total effects of a model into marginally independent elements. Visual partitioning eliminates bias of interpretation while maintaining simplicity. 

## Overall Strategy for Visual Partitioning

Recall that our paper is targeted for situations where one is attempting to fit a multivariate model, but worries some of the variables might have multiplicative effects. To identify the viability of a main-effects-only model, one can combine the aforementioned tools into a simple strategy. The goal of this strategy is to visually partition the results of a multivariate analysis. The strategy consists of five steps:

1. Identify nonlinear effects with Flexplot and loess (or quadratic) lines. 
2. Identify interactions with marginal plots. 
3. Model (and optionally test) for nonlinear/interactive effects found in Step 2.
4. Use PRPs to perform residual analysis from the model in Step 3.
5. Visualize final model partitions using PRPs. 

Each of these steps requires some elaboration, which we do in the next sections. 

### 1. Identify Nonlinear Effects With Flexplot

This step is, perhaps, the easiest. This step requires the researcher to produce a simple bivariate graphic of each variable against the outcome variable. Flexplot will default to plotting a loess line. If the line has a pattern of nonlinearity for a particular variable, a nonlinear term may need to be added to the model. 

At first glance, this strategy seems prone to bias; are we not ignoring the multivariate nature of the data if we simply plot bivariate scatterplots? For example, a nonlinear effect might appear to exist in a bivariate graphic, but disappears once we've modeled an interaction effect. Or, conversely, the bivariate plot might suggest a variable's effect is linear, but when viewed in the context of other variables, there is a nonlinear effect. 

Each of these scenarios are possible. However, we have several steps that remain that are designed to detect any sort of effects we might miss in the earlier stages. By beginning with simple bivariate plots, it gives us a straightforward starting point. 

### 2. Identify Interaction and Nonlinear Effects With Marginal Plots

Once we have identified plausible nonlinear effects, it is time to identify interactions. Recall that Flexplot allows one to visualize multiple variables simultaneously, using symbols/lines/colors and/or panels. The purpose of visualizing these simultaneously is to detect interactions. However, unless the user displays one of the variables doing the interacting on the $X$-axis, and unless the other variable doing the interacting is displayed somewhere else in the Flexplot graphic, they cannot be visually detected. As such, Step #2 is an iterative process. 

For our example data, we would alternate placing friend ideation, health, stress, and depression on the $X$-axis. Recall that depression interacts with friend ideation. To detect this interaction, friend ideation *or* depression must be on the $X$-axis, and the other variable must be displayed as separate lines/colors/symbols or in separate panels. However, marginal plots will only be able to reveal nonparallel lines in the variables represented as row/column panels. So, the following Flexplot equations would detect the interaction:

* `flexplot(ideation~depression | health + friend_ideation, data=ideation)` (slopes would vary as a function of row panels)
* `flexplot(ideation~friend_ideation + stress | depression + health, data=ideation)` (slopes would vary as a function of column panels)
* `flexplot(ideation~depression + friend_ideation | health, data=ideation)` (slopes would vary as a function of colors/symbols/lines, though the marginal plots wouldn't help detect these interactions)

On the other hand, the following Flexplot equations would *not* detect interactions:

* `flexplot(ideation~health | depression + friend_ideation, data=ideation)` (neither interacting variable is on the $X$-axis)
* `flexplot(ideation~depression | stress + health, data=ideation)` (one of the interacting variables [friend ideation] isn't displayed)

Once again, we emphasize this is an iterative process: one must take turns visualizing each variable on the $X$-axis, and one must allow every other variable in the model to be in a panel at one time or another. This may require several plots. 

There is one important caveat. Often, predictor variables are correlated with one another. For this reason, sometimes a variable that doesn't actually interact with the variable on the $X$-axis will reveal nonparallel lines when, in fact, the lines are nonparallel only because that variable is correlated with another variable with which it interacts. In these situations, it is best to identify the variable with the strongest evidence of nonparallel lines, model that, then use PRPs to see if any remaining variables contain interactions.

### 3. Model Nonlinear/Interaction Effects

Step #1 will be helpful in identifying nonlinear effects, while Step #2 will help in identifying interactions. Once they are identified, these effects should be entered into a MR model. 

One of the weaknesses of visualizations is their ambiguity.[Though, of course, ambiguity is also a strength, at least when ambiguity is merited, @Fife2019e]. It is difficult to determine from a simple graphic whether a curvilinear effect is nonlinear *enough* or if slopes are not parallel *enough* to warrant modeling them. This is where statistics come in. 

We favor a model comparison approach [@rodgers_epistemology_2010]. With this approach, one specifies a "full model," which contains the multiplicative effect (e.g., a quadratic term) and a "reduced model" (e.g., a model with main effects only). Once specified, these two models can be compared (e.g., using the `anova` function in R, or the `model.comparison` function in `flexplot`). The `model.comparison` function will report AIC, BIC, Bayes factors, $p$-values, and $R^2$. These statistics can be assist users in deciding whether the multiplicative effects are worth keeping. 

Ideally, by the end of this step, the user has a good idea of what model is most appropriate. The next step will assist in ensuring no important effects have been missed. 

### 4. Visualize Residuals Using PRPs

Recall that PRPs are a visual representation of the effect of variable(s) of interest on the outcome, after controlling for one or more covariates. Another way to think about controlling (or conditioning) is "subtraction." When we condition on one or more effects, we are *subtracting* out the effects of those variables. Presumably, when we subtract a variable's effect from the outcome, that same variable should no longer be associated with that outcome. We *should* see a flat relationship between the variable and the residuals if all effects have been modeled. If, on the other hand, that variable has a multiplicative effect and we have only modeled the additive effect, we should still see a relationship between the variable of interest and the residuals of the model. This is exactly what happened in Figure \@ref(fig:missingnonlin). 

```{r threeway, fig.cap="This plot shows a PRP where the user has modeled a two-way interaction, but there's an unmodeled three-way interaction present, as indicated by the non-parallel lines.", fig.width=6, fig.height=4.5, out.width="70%", fig.align="center"}
cormat = matrix(.3, nrow=4, ncol=4)
diag(cormat) = 1
require(tidyverse)
d = MASS::mvrnorm(1000, c(0,0,0,0), cormat) %>% data.frame %>% set_names(c("y", "a", "b", "c"))
d$y = with(d, .2*a*b + .5*a*b*c + y)
mod = lm(y~a + b + c + a:b, data=d)
p = partial_residual_plot(y~c | b + a, 
                          data=d, model=mod,
                          method="lm", suppress_model = F, 
                          labels = list("b" = c("low", "mid", "high"), 
                                        "a" = c("low", "mid", "high")))
p
```

We can use this fact to determine whether the effects we've modeled are unbiased. After the nonlinear/quadratic effects are identified (from Steps 1/2) and modeled (from Step 3), we then use PRPs in much the same we way used marginal plots in Step 2; we sequentially plot the residuals against each variable on the $X$-axis  (and the remaining variables in panels or as colors/lines/symbols) and look for flat slopes. If they're all flat and nonlinear, we have captured all signal in the model. If not, there may be missing two-way or three-way interactions, or additional nonlinear terms.

As an example, consider Figure \@ref(fig:threeway). In this example, we have simulated the data to contain both a two-way and a three-way interaction, but have only modeled the two-way interaction. The red lines are the model-implied fitted lines, while the blue lines are regression lines. (We can add regression lines to any PRP by adding the `method="lm"` argument). The partial residual plot reveals that we *still* have nonparallel slopes, even after we've subtracted out the main effects and the two-way interaction, at least in some regions of the Flexplot graphic. These results suggest there's an unmodeled three-way interaction. 

### 5. Visualize the Final Model Using PRPs

While we have written the previous sections sequentially, in actuality, this is an iterative process. However, once all PRPs show flat lines across all variables, it is time to visualize the final model using conditionally independent partitions. To do so, we must follow the following rules:

1. Variables that only show up as main effects can be visualized as a simple bivariate plot. This bivariate plot will have the predictor variable on the $X$-axis and the residuals of the model (minus the effect of the predictor variable) on the $Y$-axis.  
2. Variables that have nonlinear terms should be plotted as nonlinear. This too is displayed as a bivariate plot, but we add the main effect and nonlinear effect back into the residuals. 
3. Variables that interact must be interpreted together. This will necessitate a multivariate plot. For example, one of the variables will be on the $X$-axis and the other might be shown as column panels). The residuals on the $Y$-axis will contain the whole model minus the main effects and interaction effect of the variables that interact with one another. 
  
Of course, for each of these PRPs, the final model will be inputted into the `partial_residual_plot` function. Also, the `added_term` argument should contain every term in the linear model associated with the variable(s) of interest. For example, any variables with only main effects would simply have the main effect as the `added_term` argument (e.g., `~health`). For nonlinear terms, we would add both the linear and nonlinear terms to the `added_term` argument (e.g., `~stress + I(stress^2)`). For interaction effects, we add both the variable of interest as well as all other variables that it interacts with (e.g., `~ depression + friend_ideation + depression:friend_ideation`). For our suicide ideation dataset, our Flexplot code might look like this:


```{r, eval=FALSE, echo=TRUE}
# fit the final model
final_model = lm(ideation~health + #main effect of health
    stress + I(stress^2) + # nonlinear effect of stress
    depression * friend_ideation, # interaction effect
    data=ideation)

# plot the health effect
partial_residual_plot(ideation~health, 
              model=model, 
              added_term = ~health, 
              data=ideation)

# plot the nonlinear stress effect
partial_residual_plot(ideation~stress, 
              model=model, 
              added_term = ~stress + I(stress^2), 
              data=ideation) 

# plot the interaction effect
partial_residual_plot(ideation~friend_ideation | depression, 
                      model=model, 
                      added_term = ~depression*friend_ideation, 
                      data=ideation) 
```


# Example Analyses

For our applied analysis, we're going to base our analysis loosely on a study done by @yang2019relationships. Their paper sought to determine the relationship between depression (measured by CESD), Internet addiction (measured by the Internet Addiction Test, or IAT), and health (measured by Health Promoting Lifestyle Profile, or HPLP). More specifically, we will investigate the influence of health and Internet addiction on depression, after controlling for age, BMI, smoking status, alcohol drinking status, and religion. 

A naive analysis of these data might look like Table \@ref(tab:anovatab). This analysis suggests Internet addiction, health, and drinking have statistically significant effects. However, as we shall see, this conclusion may be premature and misleading. 

```{r anovatab}
require(flexplot)
require(tidyverse)
require(patchwork)
d = read.csv("../data/health_depression.csv")
d = d %>% dplyr::select(Age, BMI, Smoking, Drinking, Religion, CESD, IATTotalscores, HPLPTotalscores) %>% 
  set_names("Age", "BMI", "Smoking", "Drinking", "Religion", "CESD", "Internet", "Health") 

mod = lm(CESD~Health + Internet + Age + BMI + Drinking + Smoking + Religion, data=d)
aovtab = data.frame(anova(mod))
aovtab$F.value = round(aovtab$F.value, digits=2)
aovtab$F.value[8] = ""
aovtab$Pr..F. = scales::pvalue(aovtab$Pr..F.)
papaja::apa_table(aovtab, 
  caption = "Naive Analysis of the Relationship Between Health/Internet and CESD, After Controlling for Age, BMI, Drinking, Smoking, and Religion", 
  digits=2, row.names=T, 
  align = c("l", rep("c", times=ncol(aovtab))),
  col.names = c("", "DF", "SS", "MS", "F", "p"))
```

For the analyses that follow, we transformed CESD and health using a square root transformation. This not only rendered the residuals of the model more normally distributed, but it also addressed some initial problems with heteroscedasticity. 

## 1. Visualize Multiple Predictor Variables Simultaneously

As mentioned previously, this step requires us to plot each predictor variable (in our case, age, BMI, smoking status, alcohol, religion, HPLP, and Internet addiction) on the $X$-axis in order to investigate nonlinear effects. Clearly we cannot reproduce all the plots we generated for our analysis. Rather, we will provide a sample of some of the most instructive plots. 

Initially, none of the bivariate plots showed nonlinear effects. However, we discovered in the next step there was a nonlinear *and* interactive effect between Internet/CESD, and either religion or health. Figure \@ref(fig:applied1) shows a plot of the relationship between Internet addiction and CESD, for health and religion. The initial plots with loess lines (not shown) indicated a curvilinear effect. For that reason, Figure \@ref(fig:applied1) plots quadratic regression lines. It seems the effect of Internet on depression may depend on one or more of these variables (i.e., there may be an interaction present). We will investigate this further in the next section. 

```{r applied1, fig.cap="Flexplot visual of the relationship between Internet addiction and CESD, for various ages and levels of religion. The plots indicate some nonlinearity and possible interaction effects.", fig.width=6, fig.height=4, out.width="75%", fig.align="center"}
d2 = d %>% 
  mutate(CESD = sqrt(CESD), Health = sqrt(Health))
a = flexplot(CESD~Internet|Religion + Health, data=d2, method="quadratic")
a
```

## 2. Identify Interaction Effects With Marginal Plots

Figure \@ref(fig:applied2) plots the same image in Figure \@ref(fig:applied1), but with marginal plots. These images suggest the Internet effect is generally positive for each of the levels of health. Across religion, there may be evidence of a small interaction effect; the relationship is concave upward for those who are religious, while it's concave downward for those who are not religious. 

```{r applied2, fig.cap="Marginal plot of the relationship between Internet addiction and CESD, for various ages and religion levels. The row marginal plots indicate Internet may interact with religion.", fig.align="center", fig.width=8, fig.height=6, out.width="75%", }
marginal_plot(a)
```

Of course it is difficult to tell conclusively from this graph whether that interaction is worth keeping. To aid in this, we will utilize model comparisons. 

## 3. Model Nonlinear/Interaction Effects

Our original model included only main effects of all the variables. The graphic suggests we might include nonlinear terms associated with Internet, as well as an interaction with the religion variable.[^compinteraction] To test this, we will utilize model comparisons [@rodgers_epistemology_2010]. Specifically, we compared the following full and reduced models, 

[^compinteraction]: The images suggest the interaction manifests itself as a change in the coefficient for the *nonlinear* term (i.e., the quadratic component). For this reason, we need to model both the interaction term between Internet and religion, as well as the interaction between the quadratic term and religion. 

\begin{align}
\nonumber Full\,Model: \text{CESD}    &= Health + Internet + Age + BMI + Drinking + Smoking + Religion + \\
\nonumber & \qquad \qquad Internet \times Religion + Internet^2 + Internet^2 \times Religion \\
\nonumber Reduced\,Model: \text{CESD} &= Health + Internet + Age + BMI + Drinking + Smoking + Religion
\end{align}

We used `flexplot` to compute the AIC, BIC, the Bayes factor (derived from the BIC), the $p$-value from a likelihood ratio test, and the model's $R^2$. These results are presented in Table \@ref(tab:modelcomp). The AIC and $p$-value both favor the full model, while the BIC (and the BF) favor the reduced model. Recall that the BIC is more conservative than the AIC, so the fact that there's disagreement isn't surprising, especially since the full model estimates three additional parameters. Also, the full model explains an additional 2% of the variance, beyond the reduced model. For the sake of our example, we will accept the full model before moving on to the next section. 

```{r modelcomp}
Full = lm(CESD~Age + BMI + Smoking + Drinking + Health + 
     Internet + Religion + Internet:Religion +
     I(Internet^2) + I(Internet^2):Religion, data=d)
Reduced = update(Full, .~.-(Internet:Religion + I(Internet^2) + I(Internet^2):Religion))
results = model.comparison(Full, Reduced)$statistics
papaja::apa_table(results, 
                  caption = "Model Comparison of the Full and Reduced Model for the Depression, Internet Addiction, and Health Dataset.",
                  digits=2, row.names=T,
                  align = c("l", rep("c", times = ncol(results))),
                  col.names = c("", "AIC", "BIC", "BF", "p", "$R^2$"))
```

## 4. Visualize Residuals Using PRPs

Having (provisionally) chosen our full model, we will now utilize PRPs to identify whether there are any remaining effects the model has missed. As before, we will only show one representative plot to save space. Figure \@ref(fig:prpexample) shows a PRP with Internet on the $X$-axis, and health/religion in panels. The residuals of the full model are shown on the $Y$-axis. As such, we would expect to see flat lines throughout the plots if we have adequately modeled all effects. We have overlaid loess lines (blue), as well as the model-implied fit (red lines). For the most part, the two lines generate quite similar results, suggesting our model (Full Model) has extracted any signal existing between Internet, health, and religion. 

```{r prpexample, fig.cap="Partial residual plot between the residuals of CESD for the full model and Internet, health, and religion. Red lines are the model-implied fit and blue lines are loess lines.", fig.align="center", fig.width=8, fig.height=5, out.width="75%"}
library(extrafont)
loadfonts()
theme_update(text = element_text(family = "Times"))
partial_residual_plot(CESD~Internet | Health + Religion, data=d, model=Full, method="loess") +
  labs(x="Internet", y = "CESD Residuals (Full Model)") 
```

## 5. Visualize the Final Model Using PRPs

Recall that our full model has main effects for all our predictors, as well as a nonlinear effect for Internet and an interaction between the Internet effect and religion. For that reason, our final visualizations should include: 

* bivariate plots for BMI, drinking, smoking, age, and health
* a multivariate plot for Internet and religion

To save space, we chose not to plot all bivariate plots. Instead, we just plot health, Internet, and religion (since the other variables were covariates and not of theoretical interest). Figure \@ref(fig:prpfinalexample) visualizes the marginal effect of health on CESD (top plot) and the marginal effect of Internet/religion (bottom plot). These results suggest there's a very small conditional effect of health on depression. Also, Internet addiction generally increases depression, but the rate of increase is not consistent. For religious individuals, the effect of Internet on CESD is exponential: initially small changes in Internet addiction leads to relatively small increases in depression, but later changes result in increasingly larger changes in depression. For those who are not religious, on the other hand, the effect of Internet addiction on CESD tapers off; while initially small changes in Internet addiction yields large changes in depression, once these individuals reach a score of around 60, increasing Internet addiction has little effect on depression. Granted, we may be interpreting noise. (The model comparison statistics were a little ambiguous). However, these results could serve as an important starting point for subsequent studies. 

```{r prpfinalexample, fig.cap="Final partial residual plots of the internet addiction model. The left plot shows the main effect of health on CESD (after controlling for BMI, age, drinking, smoking, Internet, and religion). The right plot shows the effects of Internet/rligion on CESD (after controlling for BMI, age, drinking, smoking, and health). For the labels, A = age, D = drinking, S = smoking, H = health, I = Internet, and r = religion.", fig.width=8, fig.height=5, scale="65%", fig.align="center"}
a1 = partial_residual_plot(CESD~Health, 
                          data=d, 
                          model=Full, 
                          added_term = ~Health) +
  theme(legend.position="none") + 
  labs(x="Health", y = "CESD | BMI, A, D, S, I, and R", subtitle = "Marginal Effect of Health")
a2 = partial_residual_plot(CESD~Internet | Religion, 
                          data=d, 
                          model=Full, 
                          added_term = ~Internet + Religion + Internet:Religion +
                                              I(Internet^2) + I(Internet^2):Religion) +
  theme(legend.position="none") + 
  labs(y = "CESD | BMI, A, D, S, and H", subtitle = "Marginal Effect of Internet/Religion")

  a1 + a2 + patchwork::plot_layout(widths = c(1,2))
```


```{r finalmodests}
final_results = anova(Full) %>% 
  unclass %>% 
  data.frame %>% 
  set_names("DF", "SS", "MS", "F", "p") %>%
  mutate(semi.partial = c(estimates(Full)$semi.p, NA)) %>%
  slice(1:5) 

combined_effects = data.frame(anova(Reduced, Full))
semip = summary(Full)$r.squared - summary(Reduced)$r.squared
a = with(combined_effects, c(Df[2], Sum.of.Sq[2], Sum.of.Sq[2]/Df[2], F[2], Pr..F.[2], semip))
final_results[6,] = a
row.names(final_results) = c(row.names(anova(Full))[1:5],"Internet/Religion")
final_results = final_results %>% 
  mutate(semi.partial = round(semi.partial,digits=3), 
         p = scales::pvalue(p), 
         DF = round(DF),
         SS = round(SS, 1),
         MS = round(MS, 1),
         F = round(F, 1))
prep = round(final_results$semi.partial[6], digits=2)*100
papaja::apa_table(final_results, 
                  caption = "ANOVA Summary Table/Effect Sizes for the Final Model.",
                  row.names=T,
                  digits=c(1, 0,1,1,2,0, 3),
                  align = c("l", rep("c", times = ncol(final_results))),
                  col.names = c("", "DF", "SS", "MS", "F", "p", "Semi-partial $R^2$"))

```

For the sake of completeness, we also present the statistical estimates associated with the final model (see Table \@ref(tab:finalmodests)). Notice we have lumped together the Internet/religion effects (including the nonlinear components). We have done this because these effects should not be interpreted in isolation [@Appelbaum1974]; an interaction effect renders main effects uninterpretable (because the effect *depends* on other effects). Together, the two explain about `r prep`% of the variance. 

## Summary

The naive results presented in Table \@ref(tab:anovatab) were both routine in applied research and potentially misleading. That model suggested health, Internet addiction, and drinking have significant effects on depression. However, a deeper investigation identified potential multiplicative effects that threaten the validity of our conclusions. There is evidence to suggest the effect of Internet addiction on depression is nonlinear, and it interacts with religion in surprising and interesting ways. Our final model may be less biased, more theoretically interesting, and easier to conceptualize. 

# Discussion

Multiple regression models (including factorial ANOVAs and ANCOVAs) are likely the most popular models in psychological research. Unfortunately, they make assumptions that may be routinely violated [@Cronbach1975; @Lord1968; @Micceri1989], and assumptions in general are rarely investigated [@hoekstra_are_2012]. Two of the most problematic assumptions one can violate are linearity and homogeneity of regression [@gelman2006data]. When violated, statistical models will be biased, insights will be missed, and problematic models may proliferate in the scientific literature.

Historically, these assumptions -- when checked -- have been evaluated using brute force procedures such as stepwise regression. In this paper, we offer an alternative approach. This approach relies on visualizations to identify nonlinear and/or interactive effects, which makes the process easy and intuitive.

Perhaps more importantly, our strategy offers allows one to *simplify* complex multivariate models; one's multivariate model can be condensed, without bias, into a handful (or less) of easy-to-understand graphics which we call "visual partitions." This simplification is becoming increasingly important in the current research landscape; there's a growing mistrust of scientific results among both scientists [@Baker2016a] and laypersons [@mistrustpublic]. Visuals provide a convenient and effective solution to increasing transparency and trust of scientific results [@Tay2016a]. In other words, visual partitions offer a convenient *communication* medium that simplifies interpretation. This paper has provided tools (marginal plots, multivariate partial residual plots), strategies, and software examples. With these resources, we hope researchers will better be able to identify important effects in their models that might otherwise have been missed. 

\pagebreak

# Getting Started with Flexplot

As of this writing, Flexplot is available through Github only. It can be in stalled by first installing the devtools package:

```{r, echo=TRUE, eval=F}
install.packages("devtools")
```

With `devtools` installed, Flexplot can now be installed from Github, then loaded into the R environment:

```{r, echo=TRUE, eval=F}
devtools::install_github("dustinfife/flexplot")
require(flexplot)
```

The `flexplot` package comes pre-loaded with the suicide ideation dataset used throughout this paper. The name of the dataset is `ideation`. 


To visualize a multivariate plot, we might use the `flexplot` command as follows:

```{r, echo=TRUE, eval=F}
p = flexplot(ideation~depression | friend_ideation + stress, data=ideation)
```

Notice I'm assigning the plot to an object called `p`. This makes it easier to then use marginal plots:

```{r, echo=TRUE, eval=F}
marginal_plot(p)
```

To use partial residual plots, we have to first fit a model:

```{r, echo=TRUE, eval=F}
full_model = lm(ideation~health + 
                  stress + I(stress^2) +
                  friend_ideation*depression, 
                data=ideation)
```

(Note: `friend_ideation*depression` is a shortcut for `friend_ideation + depression + friend_ideation:depression`). 

Now we can use PRPs to visualize any of the model's relationships:

```{r, echo=TRUE, eval=F}
# visualize the health effect
partial_residual_plot(ideation~health, 
                           model=full_model,
                           data=ideation, 
                           added_term = ~health)
# visualize the stress linear and nonlinear effects
partial_residual_plot(ideation~stress, 
                           model=full_model,
                           data=ideation, 
                           added_term = ~stress + I(stress^2))
# visualize the whole model without adding terms back in
# (this is like a residual dependence plot)
partial_residual_plot(ideation~stress + health | 
                        depression + friend_ideation, 
                           model=full_model,
                           data=ideation, 
                           method="loess")
    # add a loess line to the plot to compare implied fit (red) 
    # with optimal fit
```


\pagebreak
# References